{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting The Likelihood Of An Academy Award Nomination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who doesn't love Oscar Season? Every media outlet prepares itself in advance for \"likely\" candidates to receive a nomination or this year's award and we all sit around take guesses on our likely favorites. But how do they (we) make such predictions? Is it a film's success in the box office? The ones with enormous budgets for special effects? Films directed or produced by a big name? (side note: if you ever see a film directed by William Wyler or Steven Spielberg, with 13 and 11 Best Picture nominations, respectively, it's probably a good guess that the film was/will be nominated). Or even something more mundane such as how long the movie runs, as we all know that us artsy types love our long films - looking at you Lawrence of Arabia. Predicting which films the Academy will decide on is notoriously difficult, and usually boils down to some combination of all of these factors that gives us a feeling when we see a film that \"this is the one\". As someone with a film director for a partner, and who is quite the film buff as well, trying to replicate this predictive process on a computer seemed like a fun project for merging my passion with my computer skills.\n",
    "\n",
    "Using the data from Kaggle's TMDB data set (https://www.kaggle.com/tmdb/tmdb-movie-metadata?select=tmdb_5000_credits.csv) and a list of academy award nominations (also from Kaggle: https://www.kaggle.com/unanimad/the-oscar-award?select=the_oscar_award.csv), I want to initiate the first steps of this predictive process by creating a model that can be fed some data pertaining to a film and classify if it will be nominated for Best Picture or not. Because a film's chance at actually *winning* is so dependent on the quality of other films released in its year; i.e. shockingly competitive fields like 1993, 1994, or 2015 are very different than 2021 will be after Covid closed down all theaters, we will temper our expectations and only look for nominations in the first iteration of this project.\n",
    "\n",
    "The TMDB data set is loaded with all sorts of data and, in some cases, even too much data. A lot of the fields that it provides, such as the 10th actor on screen or the crew's cook are only listed for a few films. While a forest theoretically shouldn't be biased by data that isn't useful for predictions, and while *maybe* that one cook is a great predictor of a nomination, adding all of that extra data can certainly slowed down our calculations. It's not worth one hot encode hundreds of columns to generate tens of thousands of columns of data for comparatively limited gains. To alleviate this, I called in my expert and she helped me pare down to fields that may be relevant and had enough data to justify using for this analysis. Budget, revenue, and runtime were the easy ones. We decided that we definitely needed the first four actors on screen to cover our protagonist, their two best friends, and the antagonist. Director and producer were easy adds as well. After that, feature selection became a little dicey due to sparse data and weighing how useful the other fields would be, but my partner insisted on trying to get as many heads of department as we could manage. The fields that we ultimately selected were:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- budget\n",
    "- revenue\n",
    "- runtime\n",
    "- Actor 1\n",
    "- Actor 2\n",
    "- Actor 3\n",
    "- Actor 4\n",
    "- Associate Producer\n",
    "- CG Animator\n",
    "- Casting\n",
    "- Cinematography\n",
    "- Choreographer\n",
    "- Costume Design\n",
    "- Creator\n",
    "- Director\n",
    "- Music\n",
    "- Post-Production Manager\n",
    "- Producer\n",
    "- Screenplay\n",
    "- Set Designer\n",
    "- Sound Director\n",
    "- Story\n",
    "\n",
    "We'll feed these fields into a Random Forest classifier, first with default hyperparameters, then a tuned model, and then we'll even give CatBoost a try here too to see how it performs, despite the limited number of data points for its typical use cases. At least the benefit of such small data is that we can easily play with a number of models with varying levels of iterations to compare results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing our modules and constructing our database with the included source data. The database construction and forests have been set aside in their own modules to abstract some of the code away and to make this notebook less lengthy. Feel free to browse them in the the modules section of this code repository to see how it works in more depth. Here's a quick list of what they're doing:\n",
    "\n",
    "The module imported to write the database triggers a series of other modules that parse the JSON saved as csv files in the TMDB data set and pull out a litany of fields to keep. They also normalize the data, since I figured that, while I had Python open, I may as well just do some that work in memory there.\n",
    "\n",
    "The module for pulling our data is a sql query that merges all of the data sets together and makes a few changes to make modeling easy.\n",
    "\n",
    "And finally the RF and cat_boost modules, as the names imply, are simply performing the models for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import sys\n",
    "import os\n",
    "dir_path = 'C:\\\\Users\\\\scott\\\\Desktop\\\\IMDB Project'\n",
    "sys.path.append(dir_path+'\\\\'+'modules')\n",
    "from create_database import write_tables\n",
    "from db_query import pull_data\n",
    "from random_forest_tuned import tuned_forest\n",
    "from cat_boost_v2 import cat_boost\n",
    "from random_forest_default import default_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building our database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(dir_path+'\\\\'+'Academy_Awards_Data.db')\n",
    "curs = con.cursor()\n",
    "write_tables(con, curs, dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple forest with default settings to see where we are initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Default Results -------------\n",
      "1030 total correct predictions out of 1122 total attempts (91.8%)\n",
      "3 total correct nominee predictions out of 57 total attempts (5.26%)\n",
      "1027 total correct non-nominee predictions out of 1065 total attempts (96.43%)\n",
      "Time to complete: --- 16.36 seconds ---\n",
      "--------------------------------------------\n",
      "\n",
      "          \n",
      "    \n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['91.8%', '5.26%', '96.43%', '--- 16.36 seconds ---']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pull_data(con, curs)\n",
    "iters = 100\n",
    "default_forest(data, iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some enormous overall accuracy on our first run, but it looks like we've just done a good job at predicting which movies *are not* likely to get a nomination.\n",
    "\n",
    "While this is still a great find, an enormous type II error on the positive identifications isn't exactly what I had in mind for trying to \"predict the nominees\".\n",
    "\n",
    "Perhaps we're overfitting? I'll presume that the creators of random forest didn't set the default hyperparamters for handling 20,000 columns of sparse data with only a few thousand rows. At a bare minimum, our trees will probably require some pruning, but there's likely a whole host of oddly specific changes that could be made to make our forest more effective. This might be a great time for showing off our kaggle skills and using a grid-search to tune the forest's parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Trying Other Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I won't perform the grid search here because it's quite the time consuming process to run, but you can find the code for it located in the repository for this project as \"forest_grid_search.py\".\n",
    "\n",
    "It looks like the grid search returned the following \"best\" hyperparamters:\n",
    "\n",
    "- bootstrap = False\n",
    "- class_weight = None \n",
    "- criterion = 'gini' \n",
    "- max_depth = 90 \n",
    "- max_features = 'sqrt' \n",
    "- max_leaf_nodes = None \n",
    "- min_impurity_decrease = 0.0 \n",
    "- min_impurity_split = None \n",
    "- min_samples_leaf = 1 \n",
    "- min_samples_split = 2\n",
    "- min_weight_fraction_leaf = 0.0 \n",
    "- n_jobs = None\n",
    "- oob_score = False\n",
    "- random_state = None \n",
    "- verbose = 0\n",
    "- warm_start = False\n",
    "\n",
    "Let's try plugging these into a new forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Tuned Forest Results -----------\n",
      "1015 total correct predictions out of 1122 total attempts (90.46%)\n",
      "3 total correct nominee predictions out of 57 total attempts (5.26%)\n",
      "1012 total correct non-nominee predictions out of 1065 total attempts (95.02%)\n",
      "Time to complete: --- 21.49 seconds ---\n",
      "--------------------------------------------\n",
      " \n",
      "    \n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['90.46%', '5.26%', '95.02%', '--- 21.49 seconds ---']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iters = 100\n",
    "tuned_forest(data, iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We certainly got a little bump in overall accuracy there, yet still no luck on predicting award nominations. Perhaps we should break out the big guns to see if that helps here. CatBoost mis the hottest tool on the market for classifiers these days. We'll give it a quick run on the default parameter settings to see what it returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- CatBoost Results -------------\n",
      "1006 total correct predictions out of 1122 total attempts (89.66%)\n",
      "3 total correct nominee predictions out of 57 total attempts (5.26%)\n",
      "1003 total correct non-nominee predictions out of 1065 total attempts (94.18%)\n",
      "Time to complete: --- 2.18 seconds ---\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['89.66%', '5.26%', '94.18%', '--- 2.18 seconds ---']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iters = 100\n",
    "cat_boost(data, iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was slightly *less* effective shockingly. We received the same number of correct nomination predictions, yet even fewer correct predictions of non-nominations.\n",
    "\n",
    "The time to completion for CatBoost is certainly nothing to scoff at though - achieving nearly the same accuracy in less than 1/5 of the time is remarkable. Even more impressive, is that setting up CatBoost out of the box took merely a few minutes, as it did not require the time spent setting up OHE, and it absolutely did not require the hours of pre-calculation that the grid search demanded. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## Comparing Across A Few More Iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well unfortunately, more iterations is never a substitute for missing data, as a model is only as good as the features on hand. However, we can feed various numbers of iterations into these models to see how much closer we can get our results. Below, I'll attempt each of these models with 200, 500, and 1000 iterations each.\n",
    "\n",
    "Also of note, for this particular instance with CatBoost, I was unable to find any set of hyperparameters that yielded different results than the one above, so it appears that I will leave it on the defuault hyperparameters here and see if they can yield more accuracy with further attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "200 iterations\n",
      "-------------- Default Results -------------\n",
      "1028 total correct predictions out of 1122 total attempts (91.62%)\n",
      "3 total correct nominee predictions out of 57 total attempts (5.26%)\n",
      "1025 total correct non-nominee predictions out of 1065 total attempts (96.24%)\n",
      "Time to complete: --- 23.41 seconds ---\n",
      "--------------------------------------------\n",
      "\n",
      "          \n",
      "    \n",
      "    \n",
      "------------ Tuned Forest Results -----------\n",
      "1015 total correct predictions out of 1122 total attempts (90.46%)\n",
      "3 total correct nominee predictions out of 57 total attempts (5.26%)\n",
      "1012 total correct non-nominee predictions out of 1065 total attempts (95.02%)\n",
      "Time to complete: --- 37.37 seconds ---\n",
      "--------------------------------------------\n",
      " \n",
      "    \n",
      "    \n",
      "------------- CatBoost Results -------------\n",
      "1006 total correct predictions out of 1122 total attempts (89.66%)\n",
      "3 total correct nominee predictions out of 57 total attempts (5.26%)\n",
      "1003 total correct non-nominee predictions out of 1065 total attempts (94.18%)\n",
      "Time to complete: --- 8.42 seconds ---\n",
      "--------------------------------------------\n",
      "----------------------------------------------------------------------------- \n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "500 iterations\n",
      "-------------- Default Results -------------\n",
      "1007 total correct predictions out of 1122 total attempts (89.75%)\n",
      "3 total correct nominee predictions out of 57 total attempts (5.26%)\n",
      "1004 total correct non-nominee predictions out of 1065 total attempts (94.27%)\n",
      "Time to complete: --- 48.55 seconds ---\n",
      "--------------------------------------------\n",
      "\n",
      "          \n",
      "    \n",
      "    \n",
      "------------ Tuned Forest Results -----------\n",
      "1036 total correct predictions out of 1122 total attempts (92.34%)\n",
      "3 total correct nominee predictions out of 57 total attempts (5.26%)\n",
      "1033 total correct non-nominee predictions out of 1065 total attempts (97.0%)\n",
      "Time to complete: --- 70.32 seconds ---\n",
      "--------------------------------------------\n",
      " \n",
      "    \n",
      "    \n",
      "------------- CatBoost Results -------------\n",
      "1006 total correct predictions out of 1122 total attempts (89.66%)\n",
      "3 total correct nominee predictions out of 57 total attempts (5.26%)\n",
      "1003 total correct non-nominee predictions out of 1065 total attempts (94.18%)\n",
      "Time to complete: --- 21.51 seconds ---\n",
      "--------------------------------------------\n",
      "-----------------------------------------------------------------------------  \n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "1000 iterations\n",
      "-------------- Default Results -------------\n",
      "1013 total correct predictions out of 1122 total attempts (90.29%)\n",
      "3 total correct nominee predictions out of 57 total attempts (5.26%)\n",
      "1010 total correct non-nominee predictions out of 1065 total attempts (94.84%)\n",
      "Time to complete: --- 95.55 seconds ---\n",
      "--------------------------------------------\n",
      "\n",
      "          \n",
      "    \n",
      "    \n",
      "------------ Tuned Forest Results -----------\n",
      "1041 total correct predictions out of 1122 total attempts (92.78%)\n",
      "2 total correct nominee predictions out of 57 total attempts (3.51%)\n",
      "1039 total correct non-nominee predictions out of 1065 total attempts (97.56%)\n",
      "Time to complete: --- 141.33 seconds ---\n",
      "--------------------------------------------\n",
      " \n",
      "    \n",
      "    \n",
      "------------- CatBoost Results -------------\n",
      "1006 total correct predictions out of 1122 total attempts (89.66%)\n",
      "3 total correct nominee predictions out of 57 total attempts (5.26%)\n",
      "1003 total correct non-nominee predictions out of 1065 total attempts (94.18%)\n",
      "Time to complete: --- 42.89 seconds ---\n",
      "--------------------------------------------\n",
      "-----------------------------------------------------------------------------  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "print (\"200 iterations\")\n",
    "iters = 200\n",
    "metrics.append(default_forest(data, iters))\n",
    "metrics.append(tuned_forest(data, iters))\n",
    "metrics.append(cat_boost(data, iters))\n",
    "print('''----------------------------------------------------------------------------- \n",
    "''')\n",
    "\n",
    "# Models with 500 iterations each\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "print (\"500 iterations\")\n",
    "iters = 500\n",
    "metrics.append(default_forest(data, iters))\n",
    "metrics.append(tuned_forest(data, iters))\n",
    "metrics.append(cat_boost(data, iters))\n",
    "print('''-----------------------------------------------------------------------------  \n",
    "''')\n",
    "\n",
    "# Models with 1000 iterations each\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "print (\"1000 iterations\")\n",
    "iters = 1000\n",
    "metrics.append(default_forest(data, iters))\n",
    "metrics.append(tuned_forest(data, iters))\n",
    "metrics.append(cat_boost(data, iters))\n",
    "print('''-----------------------------------------------------------------------------  \n",
    "''')\n",
    "\n",
    "# Storing it all as a dataframe and naming appropriately\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "df_metrics.rename(columns={df_metrics.columns[0]: \"Total Percent\", df_metrics.columns[1]: \"Nomination Percent\", df_metrics.columns[2]: \"Non-Nomination Percent\", df_metrics.columns[3]: \"Completion Time\"}, inplace = True)\n",
    "\n",
    "# Adding a column to label our models\n",
    "models = ['default 200 iterations', 'tuned 200 iterations', 'cat 200 iterations','default 500 iterations', 'tuned 500 iterations', 'cat 500 iterations','default 1000 iterations', 'tuned 1000 iterations', 'cat 1000 iterations' ]\n",
    "df_metrics.insert(0, \"Model\", models, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simplify this into one table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Model Total Percent Nomination Percent Non-Nomination Percent         Completion Time\n",
      "0   default 200 iterations        91.62%              5.26%                 96.24%   --- 23.41 seconds ---\n",
      "1     tuned 200 iterations        90.46%              5.26%                 95.02%   --- 37.37 seconds ---\n",
      "2       cat 200 iterations        89.66%              5.26%                 94.18%    --- 8.42 seconds ---\n",
      "3   default 500 iterations        89.75%              5.26%                 94.27%   --- 48.55 seconds ---\n",
      "4     tuned 500 iterations        92.34%              5.26%                  97.0%   --- 70.32 seconds ---\n",
      "5       cat 500 iterations        89.66%              5.26%                 94.18%   --- 21.51 seconds ---\n",
      "6  default 1000 iterations        90.29%              5.26%                 94.84%   --- 95.55 seconds ---\n",
      "7    tuned 1000 iterations        92.78%              3.51%                 97.56%  --- 141.33 seconds ---\n",
      "8      cat 1000 iterations        89.66%              5.26%                 94.18%   --- 42.89 seconds ---\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('expand_frame_repr', False)\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "\n",
    "# Lessons Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It looks like we've reached diminishing returns on the default forest after just 200 iterations. This number could potentially be sharpened a little more between 200 and 500 if we were militant about maximizing it with this particular test set, but I doubt that we would see significant returns in a generalized setting.\n",
    "\n",
    "- The tuned forest continued to grow more accurate with each test, though the percentage of nominations fell off on the last test. Who knows however if the classification of any one film is pure random chance or not. \n",
    "\n",
    "- CatBoost, like all boosted forests, are quite data hungry ventures (and perhaps even especially so compared to other boosted forests). It appears that our data may have been too sparse, or even too small for such a model, which is terribly unfortunate. Still, despite its struggles, CatBoost continues to impress with its incredible speed. The rate with which it handles turning categorical data into OHE on the back end makes an enormous difference, and even in this particular instance, may well be worth the tradeoff if this calculation were to be performed more regularly.\n",
    "\n",
    "- A more powerful model *never* beats collecting better data. This analysis should be revisited in the future with more features (preferably ones that are not as sparsely populated as some of our data) and perhaps other models, or even an ensemble of models, to see if we have more success.\n",
    "\n",
    "- It may not be easy to tell you which films are likely to receive a nomination in any given year, but we're in really good shape to tell you which ones *aren't* likely. The accuracy in predicting films that would not become Academy Award nominees is very impressive. It's unfortunate that, with such success predicting these films, that the models had such difficulty classifying in the other direction. Though, not our goal, I suppose that counts for something."
    "\n",
    "- Though not shown here, considerable time during this project was dedicated to adjusting class weights to see if I could return better results. Unfortunately, the results were less than desirable compared to weighing our false postive/false negative weights evenly. In addition to more data, future iterations of this project would likely benefit from a more robust weighing system, such as a combination of oversampling of the group that won a nomination and applying a slightly higher false negative penalty to our model."
    ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### Bonus:\n",
    "\n",
    "Here's a fun discovery I made while playing around with CatBoost after this project was complete. It turns out that it was so overtfit that its results could actually be replicated with a single iteration, and taking only 1/2 a second to run! I'm amazed to see such accuracy with a single forest, especially when compared against our RF. But this is definitely a sign that I stretched this model's limits and should feed it more data in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- CatBoost Results -------------\n",
      "1006 total correct predictions out of 1122 total attempts (89.66%)\n",
      "3 total correct nominee predictions out of 57 total attempts (5.26%)\n",
      "1003 total correct non-nominee predictions out of 1065 total attempts (94.18%)\n",
      "Time to complete: --- 0.48 seconds ---\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['89.66%', '5.26%', '94.18%', '--- 0.48 seconds ---']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iters = 1\n",
    "cat_boost(data, iters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
